{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sem Eval 2013/14 Twitter sentiment task - message polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.notebook_repr_html = True\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_colwidth = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexicon Hiu and Lu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006 4783\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Get the positiv - negative word lexicon\n",
    "#\n",
    "\n",
    "import re\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk import word_tokenize\n",
    "\n",
    "positive_lexicon_path = '/Users/gautamborgohain/PycharmProjects/DT_Labs/PLayground/Gautam_Borg/HuLiuLexicon/positive-words.txt'\n",
    "negative_lexicon_path ='/Users/gautamborgohain/PycharmProjects/DT_Labs/PLayground/Gautam_Borg/HuLiuLexicon/negative-words.txt'\n",
    "poshand = open(positive_lexicon_path)\n",
    "neghand = open(negative_lexicon_path)\n",
    "poslist = []\n",
    "neglist = []\n",
    "for line in poshand:\n",
    "    poslist.append(re.sub(r'\\n','',line))\n",
    "for line in neghand:\n",
    "    neglist.append(re.sub(r'\\n','',line))\n",
    "print(len(poslist),len(neglist))\n",
    "\n",
    "poshand.close()\n",
    "neghand.close()\n",
    "\n",
    "def getPositiveWordCount(tweet):\n",
    "    countPos = 0\n",
    "    for word in word_tokenize(tweet):\n",
    "        if len(word)>=2 and word in poslist: countPos+=1\n",
    "    return countPos\n",
    "\n",
    "def getNegativeWordCount(tweet):\n",
    "    countNeg = 0\n",
    "    for word in word_tokenize(tweet):\n",
    "        if len(word)>=2 and word in neglist: countNeg+=1\n",
    "    return countNeg\n",
    "\n",
    "class lexiconSent(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform(self, frame):\n",
    "        df = pd.DataFrame(columns=['POS_LEX','NEG_LEX'])\n",
    "        df['POS_LEX'] =   [getPositiveWordCount(tweet) for tweet in frame]\n",
    "        df['NEG_LEX'] =   [getNegativeWordCount(tweet) for tweet in frame]\n",
    "             \n",
    "        return df.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Puctuations -  Exclamations, quotes and question marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class punctuations(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, frame):\n",
    "        df = pd.DataFrame()\n",
    "        punc_excl = []\n",
    "        punc_ques = []\n",
    "        punc_quot = []\n",
    "        for tweet in frame:\n",
    "            punc_excl.append(len(re.findall(r'!',tweet)))\n",
    "            punc_ques.append(len(re.findall(r'\\?',tweet)))\n",
    "            punc_quot.append(len(re.findall(r'\\'|\"',tweet)))\n",
    "\n",
    "        df['PUNC_EXCL'] = punc_excl\n",
    "        df['PUNC_QUES'] = punc_ques\n",
    "        df['PUNC_QUOT'] = punc_quot\n",
    "        return df.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the training and the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2111\n",
      "1137\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "Difference in the data\n",
    "\n",
    "\n",
    "This dataset contains around 3200 missign tweets\n",
    "\n",
    "'''\n",
    "\n",
    "orgtrain = pd.read_csv('/Users/gautamborgohain/Desktop/SEmEval_train.tsv',sep='\\t')\n",
    "orgtrain.columns = ['ID','ID2','Sentiment','Tweet']\n",
    "orgtest = pd.read_excel('/Users/gautamborgohain/Google Drive/CI/twitter_download20160410.xlsx')\n",
    "print(len(orgtrain[orgtrain.Tweet == 'Not Available']))\n",
    "print(len(orgtest[orgtest.Tweet == 'Not Available']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/ipykernel/__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/ipykernel/__main__.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       " 0    3676\n",
       " 1    2814\n",
       "-1    1063\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semdf_train = pd.read_csv('/Users/gautamborgohain/Desktop/SEmEval_train.tsv',sep='\\t')\n",
    "semdf_train.columns = ['ID','ID2','Sentiment','Tweet']\n",
    "semdf_train = semdf_train[semdf_train.Tweet != 'Not Available']\n",
    "semdf_train.Sentiment[semdf_train.Sentiment == 'neutral'] = 0\n",
    "semdf_train.Sentiment[semdf_train.Sentiment == 'positive'] = 1\n",
    "semdf_train.Sentiment[semdf_train.Sentiment == 'negative'] = -1\n",
    "semdf_train.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0    3548\n",
       " 1    2970\n",
       "-1    1331\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semdf_test = pd.read_excel('/Users/gautamborgohain/Google Drive/CI/twitter_download20160410.xlsx')\n",
    "# semdf_test.columns = ['ID','ID2','Sentiment','Tweet']\n",
    "semdf_test = semdf_test[semdf_test.Tweet != 'Not Available']\n",
    "semdf_test.Sentiment[semdf_test.Sentiment == 'neutral'] = 0\n",
    "semdf_test.Sentiment[semdf_test.Sentiment == 'positive'] = 1\n",
    "semdf_test.Sentiment[semdf_test.Sentiment == 'negative'] = -1\n",
    "semdf_train.Sentiment =  [int(pred) for pred in semdf_train.Sentiment]\n",
    "semdf_test.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>USER ID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>282031301962395008</td>\n",
       "      <td>T14111200</td>\n",
       "      <td>0</td>\n",
       "      <td>dec 21st 2012 will be know not as the end of the world but the Baby Boom! #2012shit</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11975</td>\n",
       "      <td>SM112166</td>\n",
       "      <td>-1</td>\n",
       "      <td>Yar he quite clever but aft many guesses lor. He got ask me 2 bring but i thk darren not so will...</td>\n",
       "      <td>Yes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>136592</td>\n",
       "      <td>LJ112295</td>\n",
       "      <td>-1</td>\n",
       "      <td>Yeah we have Thin Lizzy here I HATE the informercials !</td>\n",
       "      <td>Not Available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>253421252956544992</td>\n",
       "      <td>T13114433</td>\n",
       "      <td>0</td>\n",
       "      <td>MT @LccSy #Syria, Deir Ezzor: Ali Bashar al-theeb was martyred. He was a soldier of the Free Syr...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>220880422320603008</td>\n",
       "      <td>T14114138</td>\n",
       "      <td>-1</td>\n",
       "      <td>@MacMiller hate my life, because i can't see you at the roskilde festival on saturday, promise m...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID    USER ID Sentiment  \\\n",
       "1  282031301962395008  T14111200         0   \n",
       "2               11975   SM112166        -1   \n",
       "3              136592   LJ112295        -1   \n",
       "4  253421252956544992  T13114433         0   \n",
       "5  220880422320603008  T14114138        -1   \n",
       "\n",
       "                                                                                                 Tweet  \\\n",
       "1                  dec 21st 2012 will be know not as the end of the world but the Baby Boom! #2012shit   \n",
       "2  Yar he quite clever but aft many guesses lor. He got ask me 2 bring but i thk darren not so will...   \n",
       "3                                              Yeah we have Thin Lizzy here I HATE the informercials !   \n",
       "4  MT @LccSy #Syria, Deir Ezzor: Ali Bashar al-theeb was martyred. He was a soldier of the Free Syr...   \n",
       "5  @MacMiller hate my life, because i can't see you at the roskilde festival on saturday, promise m...   \n",
       "\n",
       "      Unnamed: 4  \n",
       "1            NaN  \n",
       "2           Yes.  \n",
       "3  Not Available  \n",
       "4            NaN  \n",
       "5            NaN  "
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semdf_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# filtering out the sms and the live journal dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2093 1142 4614\n"
     ]
    }
   ],
   "source": [
    "sms = semdf_test[semdf_test['USER ID'].str.startswith('SM')]\n",
    "lj = semdf_test[semdf_test['USER ID'].str.startswith('LJ')]\n",
    "t = semdf_test[semdf_test['USER ID'].str.startswith('T')]\n",
    "\n",
    "print(len(sms),len(lj),len(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    2051\n",
       " 0    1930\n",
       "-1     633\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes. I am testing my models on the test set provided in the Task and not using the development set. Shoot me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = semdf_train.Tweet\n",
    "y = [int(y) for y in semdf_train.Sentiment]\n",
    "\n",
    "Xt = t.Tweet\n",
    "yt = [int(y) for y in t.Sentiment]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base line score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.630905938448\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.66      0.16      0.26       633\n",
      "          0       0.56      0.90      0.69      1930\n",
      "          1       0.80      0.52      0.63      2051\n",
      "\n",
      "avg / total       0.68      0.63      0.60      4614\n",
      "\n",
      "0.446215783403\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_recall_curve, classification_report\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "                    ('bow', CountVectorizer(stop_words='english',lowercase=True,ngram_range=(1,4)))\n",
    "\n",
    "                ])\n",
    "    ),\n",
    "    ('svm', LinearSVC())    \n",
    "    ])\n",
    "\n",
    "pipeline.fit(X,y)\n",
    "predictions  = pipeline.predict(Xt)\n",
    "print(accuracy_score(predictions, yt)) \n",
    "print(classification_report(yt,predictions))\n",
    "print(f1_score(predictions,yt,average='macro', labels = [1,-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the common words in the training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Added the Punctuation and Lexicon features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.672060135049\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "                    ('bow', CountVectorizer(stop_words='english',lowercase=True,ngram_range=(1,4))),\n",
    "                    ('lex',lexiconSent()),\n",
    "                    ('punc',punctuations()),\n",
    "\n",
    "                ])\n",
    "    ),\n",
    "    ('svm', LinearSVC())    \n",
    "    ])\n",
    "\n",
    "pipeline.fit(X,y)\n",
    "predictions  = pipeline.predict(Xt)\n",
    "print(accuracy_score(predictions, yt)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lengths\n",
    "\n",
    "Length of the tweet and the lengths of the largest and the smallest words in the tweet and the average length of words in the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.044522437723423"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "tweet = 'sdfdf fdff g g g                                gkkl'\n",
    "tweet = re.sub(r'\\s+',' ',tweet)\n",
    "math.log(len(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class lengths(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, frame):\n",
    "        df = pd.DataFrame()\n",
    "        df['Length'] = [len(tweet) for tweet in frame]\n",
    "        df['Length_words_MAX'] = [max([len(part) for part in tweet.split(' ')]) for tweet in frame]\n",
    "        df['Length_words_MIN'] = [max([len(part) for part in re.sub(r'\\s+',' ',tweet).split(' ')]) for tweet in frame]\n",
    "        df['Length_words_AVG'] = [np.mean([len(part) for part in tweet.split(' ')]) for tweet in frame]\n",
    "        return df.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regular expression to clean up the tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class regExProcesses(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        tweets = []\n",
    "        for tweet in df:\n",
    "#             tweet = re.sub('tomorrow|rt','',tweet)\n",
    "            tweet = re.sub(r'&gt;|&amp;|&lt;','',tweet)\n",
    "            tweet = re.sub(r'[\\n]','',tweet)\n",
    "            tweet = re.sub('[\\.]+', '.', tweet)\n",
    "            tweet = re.sub('@[^\\s]*', '', tweet)\n",
    "            tweet = re.sub('((www\\.[^ ]+)|(https?://[^ ]+))', '', tweet)\n",
    "            tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "            tweets.append(tweet)\n",
    "            \n",
    "        return tweets\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPQA Subjectivity lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleantweet(tweet):\n",
    "    tweet = re.sub('url|at_user|rt|\\.', '', tweet)  ## removing these from the tweets\n",
    "    return tweet\n",
    "\n",
    "class subjectivityLexicon(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        lexicon = pd.read_csv(lexicon_path)\n",
    "        tweet_tags = []\n",
    "        count_tweet = 1\n",
    "        for tweet in df:\n",
    "            tweet = cleantweet(tweet)\n",
    "            typeList = []\n",
    "            priorpolarityList = []\n",
    "            count_word = 0  \n",
    "            count_tweet += 1\n",
    "            for word in word_tokenize(tweet):\n",
    "                result = lexicon[lexicon.word1 == word]\n",
    "                if len(result) != 0:  # word is there in the lexicon\n",
    "#                     if len(result) == 1:  # this case is handling the ones where the there is only one record of the word\n",
    "                        typeList.append(result.iloc[0][0])\n",
    "                        priorpolarityList.append(result.iloc[0][5])\n",
    "#                     if len(result) > 1:  \n",
    "#     #                     print('Have to tag POS, Hold On!')\n",
    "#                         poslist = pos_tag(word_tokenize(tweet))#Tag the tweet\n",
    "#                         postag = poslist[count_word][1]#Using the position of the word, find the POS tag\n",
    "#                         if postag in ['NN', 'NNP', 'NNS',\n",
    "#                                       'NNPS']:  # make the POS tags to the format used by the MPQA lexicon\n",
    "#                             postag = 'noun'\n",
    "#                         elif postag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
    "#                             postag = 'verb'\n",
    "#                         elif postag in ['RB', 'RBR', 'RBS']:\n",
    "#                             postag = 'adverb'\n",
    "#                         elif postag in ['JJ', 'JJR', 'JJS']:\n",
    "#                             postag = 'adj'\n",
    "#                         second_result = result[result.pos1 == postag]#get the word-postag combination from the lexicon\n",
    "#                         if len(second_result) != 0:  \n",
    "#                             typeList.append(second_result.iloc[0][0])\n",
    "#                             priorpolarityList.append(second_result.iloc[0][5])\n",
    "\n",
    "                count_word += 1\n",
    "\n",
    "            tweet_tags.append(' '.join(typeList) + ' ' + ' '.join(priorpolarityList))\n",
    "\n",
    "        return tweet_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lexicon_path = '/Users/gautamborgohain/PycharmProjects/Twitter_target_dependent_SA/subjectivity.csv'\n",
    "\n",
    "class subjectivityLexicon(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        lexicon = pd.read_csv(lexicon_path)\n",
    "        tweet_tags = []\n",
    "        count_tweet = 1\n",
    "        for tweet in df:\n",
    "            priorpolarityList = [lexicon[lexicon.word1 == word].priorpolarity if (len(lexicon[lexicon.word1 == word].priorpolarity) != 0) else 'None' for word in word_tokenize(tweet)]\n",
    "            typeList = [lexicon[lexicon.word1 == word].type if (len(lexicon[lexicon.word1 == word].type) != 0) else 'None' for word in word_tokenize(tweet)]\n",
    "            tweet_tags.append(' '.join(typeList) + ' ' + ' '.join(priorpolarityList))\n",
    "\n",
    "        return tweet_tags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negations -  if its is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class negations(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, frame):\n",
    "        df = pd.DataFrame()\n",
    "        dons = []\n",
    "        nots = []\n",
    "        for tweet in frame:\n",
    "            dons.append(len(re.findall(' don | dont | don\\'t | never | no | not | neither | nor | none ',tweet)))\n",
    "            nots.append(len(re.findall(' not ',tweet))) \n",
    "        \n",
    "        df = pd.DataFrame({'dons' : dons,'nots': nots})\n",
    "        return df.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class emoticons(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, frame):\n",
    "        df = pd.DataFrame()\n",
    "        posEmot,negEmot,posEmo,negEmo,neutralEmo = [],[],[],[],[]\n",
    "        for tweet in frame:\n",
    "            posEmot.append(len(re.findall(r':-D|:D|:-\\)|:\\)|;-\\)|;\\)',tweet)))\n",
    "            negEmot.append(len(re.findall(r':-\\(|:\\(',tweet)))\n",
    "            \n",
    "        df['EMOT_POS'] = posEmot\n",
    "        df['EMOT_NEG'] = negEmot\n",
    "        \n",
    "        return df.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word elongations - 3 letters consecutive\n",
    "\n",
    "- The no of elongations in the tweet\n",
    "- The average of the elongation length in the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class elongation(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, frame):\n",
    "        df = pd.DataFrame()\n",
    "        no_elong = []\n",
    "        avg_elong = []\n",
    "        reg = re.compile('([a-zA-Z])\\\\1{3,}')\n",
    "        for tweet in frame:\n",
    "            elongs = []\n",
    "            no_elong.append(len(reg.findall(tweet)))\n",
    "            for match in reg.finditer(tweet):\n",
    "                                     elongs.append((match.end() - match.start()))\n",
    "                                     \n",
    "            avg_elong.append(np.mean(elongs))                         \n",
    "        df['No_Elong'] = no_elong\n",
    "        df['Avg_Elong'] = avg_elong\n",
    "        df.fillna(value=0,inplace=True)\n",
    "        return df.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "tweet = 'I am sooooo going to killllllll yoiu'\n",
    "for match in re.finditer(\"([a-zA-Z])\\\\1{2,}\",tweet):\n",
    "    print(match.end() - match.start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.\n",
      "  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No_Elong</th>\n",
       "      <th>Avg_Elong</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.226667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.342893</td>\n",
       "      <td>1.189064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         No_Elong   Avg_Elong\n",
       "count  100.000000  100.000000\n",
       "mean     0.060000    0.226667\n",
       "std      0.342893    1.189064\n",
       "min      0.000000    0.000000\n",
       "25%      0.000000    0.000000\n",
       "50%      0.000000    0.000000\n",
       "75%      0.000000    0.000000\n",
       "max      3.000000    9.000000"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " def elongs(frame):\n",
    "        df = pd.DataFrame()\n",
    "        no_elong = []\n",
    "        avg_elong = []\n",
    "        reg = re.compile('([a-zA-Z])\\\\1{3,}')\n",
    "        for tweet in frame:\n",
    "            elongs = []\n",
    "            no_elong.append(len(reg.findall(tweet)))\n",
    "            for match in reg.finditer(tweet):\n",
    "                                     elongs.append((match.end() - match.start()))\n",
    "                                     \n",
    "            avg_elong.append(np.mean(elongs))                         \n",
    "        df['No_Elong'] = no_elong\n",
    "        df['Avg_Elong'] = avg_elong\n",
    "        df.fillna(value=0,inplace=True)\n",
    "        return df\n",
    "    \n",
    "df = elongs(X[0:100])\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hastags in the tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class hash_tag(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, frame):\n",
    "        return [' '.join(re.findall(r'#[^\\s]+',tweet)) for tweet in frame]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HASH_POS</th>\n",
       "      <th>HASH_NEG</th>\n",
       "      <th>HASH_ALL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7553</td>\n",
       "      <td>7553</td>\n",
       "      <td>7553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>7553</td>\n",
       "      <td>7553</td>\n",
       "      <td>6022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       HASH_POS HASH_NEG HASH_ALL\n",
       "count      7553     7553     7553\n",
       "unique        1        1     1397\n",
       "top                              \n",
       "freq       7553     7553     6022"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hash_vect(frame):\n",
    "        df = pd.DataFrame()\n",
    "        hash_pos_all = []\n",
    "        hash_neg_all = []\n",
    "        hash_all = []\n",
    "        for tweet in frame:\n",
    "            hash_pos = []\n",
    "            hash_neg = []\n",
    "            hash_inTweet = []\n",
    "            hashs = re.findall(r'#[^\\s]+',tweet)\n",
    "            for hashe in hashs:\n",
    "                if hashe in poslist:\n",
    "                    hash_pos.append(hashe)\n",
    "                if hashe in neglist:\n",
    "                    hash_neg.append(hashe)\n",
    "                hash_inTweet.append(hashe)\n",
    "            hash_pos_all.append(' '.join(hash_pos))\n",
    "            hash_neg_all.append(' '.join(hash_neg))  \n",
    "            hash_all.append(' '.join(hash_inTweet))        \n",
    "        df['HASH_POS'] = hash_pos_all\n",
    "        df['HASH_NEG'] = hash_neg_all\n",
    "        df['HASH_ALL'] = hash_all        \n",
    "        return df\n",
    "df = hash_vect(X)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lexicon does not have the sentiment if any of the hash tags in the training set, so just taking the hash tags as is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'stressfull' in neglist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class urls_flag(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, frame):\n",
    "        df = pd.DataFrame()\n",
    "        urls = []\n",
    "        for tweet in frame:\n",
    "            urls.append(len(re.findall('((www\\.[^ ]+)|(https?://[^ ]+))', tweet)))\n",
    "        df['URLS'] = urls\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = 'I think I may have a heart attack for Jason Wus new collection. So Charlotte Rampling in the Night  http://t.co/2KRW4kTn'\n",
    "len(re.findall('((www\\.[^ ]+)|(https?://[^ ]+))', tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7553\n"
     ]
    }
   ],
   "source": [
    "def urls(frame):\n",
    "    df = pd.DataFrame()\n",
    "    urls = []\n",
    "    for tweet in frame:\n",
    "        urls.append(len(re.findall('((www\\.[^ ]+)|(https?://[^ ]+))', tweet)))\n",
    "    df['URLS'] = urls\n",
    "    return df\n",
    "print(len(X))\n",
    "df = urls(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class remove_url(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, frame):\n",
    "        return [re.sub('((www\\.[^ ]+)|(https?://[^ ]+))','',tweet) for tweet in frame]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "class posTagTweets(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        tagsoftweet = []\n",
    "        for tweet in df:\n",
    "            postaggedtweet = pos_tag(word_tokenize(tweet))  # this one is pos atgged..list inside list : token[1] for tag\n",
    "            tags = []\n",
    "            for token in postaggedtweet:\n",
    "                tags.append(token[1])\n",
    "            tagsoftweet.append(' '.join(tags))\n",
    "#             print(' '.join(tags))\n",
    "            \n",
    "#         df = getFeatureDF(tagsoftweet)\n",
    "        return tagsoftweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class otherFeats(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, frame):\n",
    "        df = pd.DataFrame()\n",
    "        no_of_sents = []\n",
    "        no_of_newlines = []\n",
    "        no_of_rsdots = []\n",
    "        no_of_mentions = []\n",
    "        no_of_rspaces = []\n",
    "        for tweet in frame:\n",
    "            no_of_sents.append(len(tweet.split('.')))\n",
    "            no_of_newlines.append(len(re.findall(r'[\\n]',tweet)))\n",
    "            no_of_rsdots.append(len(re.findall('[\\.]+', tweet)))\n",
    "            no_of_mentions.append(len(re.findall('@[^\\s]*', tweet)))\n",
    "            no_of_rspaces.append(len(re.findall('[\\s]+', tweet)))\n",
    "            \n",
    "        df['SENTS'] = no_of_sents\n",
    "        df['NEWLINES'] = no_of_newlines\n",
    "        df['DOTS'] = no_of_rsdots\n",
    "        df['MENTIONS'] = no_of_mentions\n",
    "        df['SPACES'] = no_of_rspaces\n",
    "        return df.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>preds</th>\n",
       "      <th>-1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yt</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>566</td>\n",
       "      <td>626</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>157</td>\n",
       "      <td>3097</td>\n",
       "      <td>294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>181</td>\n",
       "      <td>1091</td>\n",
       "      <td>1698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "preds   -1     0     1\n",
       "yt                    \n",
       "-1     566   626   139\n",
       " 0     157  3097   294\n",
       " 1     181  1091  1698"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=  pd.DataFrame({'yt': yt,'preds' : predictions})\n",
    "pd.crosstab(df.yt,df.preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.668617251842\n",
      "0.617283050158\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.61      0.38      0.47       633\n",
      "          0       0.61      0.83      0.70      1930\n",
      "          1       0.78      0.61      0.68      2051\n",
      "\n",
      "avg / total       0.68      0.67      0.66      4614\n",
      "\n",
      "0.573591828758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.\n",
      "  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "pipeline = Pipeline([\n",
    "#     ('regex', regExProcesses()),\n",
    "    ('features', FeatureUnion([\n",
    "                    ('bow', Pipeline([\n",
    "                                ('clean',remove_url()),\n",
    "                                ('vect',CountVectorizer(stop_words='english',lowercase=True,ngram_range=(1,4),tokenizer=TreebankWordTokenizer().tokenize))\n",
    "                                ])),\n",
    "                    ('tfbow', Pipeline([\n",
    "                                    ('clean',remove_url()),\n",
    "                                    ('tempvect', CountVectorizer()),\n",
    "                                    ('tfvect',TfidfTransformer())\n",
    "                                ])),                    \n",
    "                    ('cow', Pipeline([\n",
    "                                    ('clean',remove_url()),\n",
    "                                    ('cvect',CountVectorizer(analyzer = 'char_wb',lowercase=True,ngram_range=(2,2)))\n",
    "                                ])),\n",
    "                    ('lex',lexiconSent()),\n",
    "                    ('punc',punctuations()),\n",
    "                    ('len',lengths()),\n",
    "                    ('negs',negations()),\n",
    "                    ('emots',emoticons()),\n",
    "                    ('elongs',elongation()),\n",
    "                    ('hashs',Pipeline([\n",
    "                                ('hashs',hash_tag()),\n",
    "                                ('hash_vect',CountVectorizer())\n",
    "                            ])),\n",
    "                    ('urls',urls_flag()),\n",
    "                    ('negativs', negatives()),\n",
    "                    ('counts',otherFeats())\n",
    "\n",
    "                ])\n",
    "    ),\n",
    "    ('svm', LinearSVC())    \n",
    "    ])\n",
    "\n",
    "pipeline.fit(X,y)\n",
    "predictions  = pipeline.predict(Xt)\n",
    "print(accuracy_score(predictions, yt)) \n",
    "print(f1_score(predictions,yt,average='macro'))\n",
    "print(classification_report(yt,predictions))\n",
    "print(f1_score(predictions,yt,average='macro', labels = [1,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.652795838752\n",
      "0.583511220486\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.70      0.28      0.40       633\n",
      "          0       0.57      0.91      0.70      1930\n",
      "          1       0.83      0.52      0.64      2051\n",
      "\n",
      "avg / total       0.71      0.65      0.64      4614\n",
      "\n",
      "0.523266830729\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "pipeline = Pipeline([\n",
    "#     ('regex', regExProcesses()),\n",
    "    ('features', FeatureUnion([\n",
    "                    ('bow', Pipeline([\n",
    "                                ('clean',remove_url()),\n",
    "                                ('vect',CountVectorizer(stop_words='english',lowercase=True,ngram_range=(1,4),tokenizer=TreebankWordTokenizer().tokenize))\n",
    "                                ])),\n",
    "#                     ('tfbow', Pipeline([\n",
    "#                                     ('clean',remove_url()),\n",
    "#                                     ('tempvect', CountVectorizer()),\n",
    "#                                     ('tfvect',TfidfTransformer())\n",
    "#                                 ])),                    \n",
    "#                     ('cow', Pipeline([\n",
    "#                                     ('clean',remove_url()),\n",
    "#                                     ('cvect',CountVectorizer(analyzer = 'char_wb',lowercase=True,ngram_range=(2,2)))\n",
    "#                                 ])),\n",
    "                    ('lex',lexiconSent()),\n",
    "                    ('punc',punctuations()),\n",
    "                    ('len',lengths()),\n",
    "#                     ('negs',negations()),\n",
    "                    ('emots',emoticons()),\n",
    "#                     ('elongs',elongation()),\n",
    "                    ('hashs',Pipeline([\n",
    "                                ('hashs',hash_tag()),\n",
    "                                ('hash_vect',CountVectorizer())\n",
    "                            ])),\n",
    "                    ('urls',urls_flag()),\n",
    "#                     ('negativs', negatives()),\n",
    "#                     ('counts',otherFeats())\n",
    "\n",
    "                ])\n",
    "    ),\n",
    "    ('svm', LinearSVC())    \n",
    "    ])\n",
    "\n",
    "pipeline.fit(X,y)\n",
    "predictions  = pipeline.predict(Xt)\n",
    "print(accuracy_score(predictions, yt)) \n",
    "print(f1_score(predictions,yt,average='macro'))\n",
    "print(classification_report(yt,predictions))\n",
    "print(f1_score(predictions,yt,average='macro', labels = [1,-1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
