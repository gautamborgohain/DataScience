{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Dependent Twitter Sentiment Predictions\n",
    "\n",
    "By Gautam Borgohain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All imports\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_recall_curve, classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import cross_validation\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "lexicon_path = '/Users/gautamborgohain/PycharmProjects/DataScience/Twitter_target_dependent_SA/subjectivity.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = None\n",
    "pd.options.display.notebook_repr_html = True\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "data = pd.read_excel('/Users/gautamborgohain/Desktop/Tweets_labeled_325.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.index = np.arange(len(data))\n",
    "y = data.Sentiment\n",
    "\n",
    "X = data.Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Funciton to create the dummy variableso of the features that are extracted\n",
    "\n",
    "def getFeatureDF(feature_list):\n",
    "    vectorizer = CountVectorizer()\n",
    "    docmatrix = vectorizer.fit_transform(feature_list).toarray()\n",
    "    columns = vectorizer.get_feature_names()\n",
    "    columns = [word.upper() for word in columns]  # uppercasing to avoid conflict of in and other words\n",
    "    df = pd.DataFrame(data=docmatrix, columns=columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regex word normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def regexStuff(tweet):\n",
    "#     tweet = re.sub(r'@SMRT_singapore|@smrt_singapore|@smrt|@SMRT_Singapore','TARGET',tweet)\n",
    "    tweet = re.sub(r'@[^ ]*','TARGET',tweet)\n",
    "    #Clear the http and other characters that are causing problems\n",
    "    tweet = re.sub('((www\\.[^ ]+)|(https?://[^ ]+))', '', tweet)\n",
    "    tweet = re.sub(r'http?[^ ]+','',tweet)\n",
    "    tweet = re.sub(r'[\\n]','',tweet)\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    tweet = re.sub('[\\.]+', '.', tweet)\n",
    "    tweet = re.sub('…','',tweet)\n",
    "    tweet = re.sub('[-—]','',tweet)\n",
    "    tweet = re.sub(r'&gt;|&amp;|&lt;','',tweet)\n",
    "    #Substitue common shorthands with the appropriate words for POS tagging and type dependecy to work\n",
    "    tweet = re.sub(r' u ',' you ',tweet)\n",
    "    tweet = re.sub(r' n ',' no ',tweet)\n",
    "    tweet = re.sub(r' y ',' why ',tweet)\n",
    "    tweet = re.sub(r' nt ',' not ',tweet)\n",
    "    tweet = re.sub(r' dwn ',' down ',tweet)\n",
    "    tweet = re.sub(r' frver ',' forever ',tweet)\n",
    "    tweet = re.sub(r' bc ',' because ',tweet)\n",
    "    tweet = re.sub(r' bcoz ',' because ',tweet)\n",
    "    tweet = re.sub(r' cuz ',' because ',tweet)\n",
    "    tweet = re.sub(r' im ',' I am ',tweet)\n",
    "    tweet = re.sub(r' zzz ',' sleep ',tweet)\n",
    "    tweet = re.sub(r' y\\'all ',' you all ',tweet)\n",
    "    #Set the hash tags\n",
    "    tweet = re.sub(r'#','HASH_',tweet)\n",
    "    return tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class regExProcesses(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        return [regexStuff(tweet) for tweet in df]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "class posTagTweets(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        tagsoftweet = []\n",
    "        for tweet in df:\n",
    "            postaggedtweet = pos_tag(word_tokenize(tweet))  # this one is pos atgged..list inside list : token[1] for tag\n",
    "            tags = []\n",
    "            for token in postaggedtweet:\n",
    "                tags.append(token[1])\n",
    "            tagsoftweet.append(' '.join(tags))\n",
    "#             print(' '.join(tags))\n",
    "            \n",
    "#         df = getFeatureDF(tagsoftweet)\n",
    "        return tagsoftweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subjectivity Lexicon - Wilson et al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleantweet(tweet):\n",
    "    tweet = re.sub('url|at_user|rt|\\.', '', tweet)  ## removing these from the tweets\n",
    "    return tweet\n",
    "\n",
    "class subjectivityLexicon(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        lexicon = pd.read_csv(lexicon_path)\n",
    "        tweet_tags = []\n",
    "        count_tweet = 1\n",
    "        for tweet in df:\n",
    "            tweet = cleantweet(tweet)\n",
    "            typeList = []\n",
    "            priorpolarityList = []\n",
    "            count_word = 0  \n",
    "            count_tweet += 1\n",
    "            for word in word_tokenize(tweet):\n",
    "                result = lexicon[lexicon.word1 == word]\n",
    "                if len(result) != 0:  # word is there in the lexicon\n",
    "                    if len(result) == 1:  # this case is handling the ones where the there is only one record of the word\n",
    "                        typeList.append(result.iloc[0][0])\n",
    "                        priorpolarityList.append(result.iloc[0][5])\n",
    "                    if len(result) > 1:  \n",
    "    #                     print('Have to tag POS, Hold On!')\n",
    "                        poslist = pos_tag(word_tokenize(tweet))#Tag the tweet\n",
    "                        postag = poslist[count_word][1]#Using the position of the word, find the POS tag\n",
    "                        if postag in ['NN', 'NNP', 'NNS',\n",
    "                                      'NNPS']:  # make the POS tags to the format used by the MPQA lexicon\n",
    "                            postag = 'noun'\n",
    "                        elif postag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
    "                            postag = 'verb'\n",
    "                        elif postag in ['RB', 'RBR', 'RBS']:\n",
    "                            postag = 'adverb'\n",
    "                        elif postag in ['JJ', 'JJR', 'JJS']:\n",
    "                            postag = 'adj'\n",
    "                        second_result = result[result.pos1 == postag]#get the word-postag combination from the lexicon\n",
    "                        if len(second_result) != 0:  \n",
    "                            typeList.append(second_result.iloc[0][0])\n",
    "                            priorpolarityList.append(second_result.iloc[0][5])\n",
    "\n",
    "                count_word += 1\n",
    "\n",
    "            tweet_tags.append(' '.join(typeList) + ' ' + ' '.join(priorpolarityList))\n",
    "\n",
    "        return tweet_tags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keywords = ['SMRT','smrt','smrt_singapore','SMRT_SINGAPORE','train','mrt','TARGET','people','LRT','lrt']\n",
    "\n",
    "def get_hastags(tweet):\n",
    "    hash_tags = re.findall('HASH_([^ ]*)', tweet)\n",
    "    return hash_tags\n",
    "\n",
    "def getAdjectives(tweet):\n",
    "    poslist = pos_tag(word_tokenize(tweet))\n",
    "    adjectives = []\n",
    "    for pos in poslist:\n",
    "        if pos[1] in ['JJ', 'JJR', 'JJS']:\n",
    "            adjectives.append(pos[0])\n",
    "            \n",
    "    return adjectives\n",
    "            \n",
    "\n",
    "class enhancedTargetFeats(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, frame):\n",
    "        tweet_target_features = []\n",
    "        for tweet in frame:\n",
    "            tweet = cleantweet(tweet)\n",
    "            tags = get_hastags(tweet)\n",
    "    #         keywords = ['SMRT', 'mrt','lrt','LRT', 'MRT', 'smrt', 'Singapore_MRT',\"TARGET\"]\n",
    "            tokens = word_tokenize(tweet)  \n",
    "            targets_feature = []\n",
    "            for keyword in keywords:\n",
    "                if keyword in tags:  \n",
    "                    feature = keyword + '_hash'\n",
    "                    targets_feature.append(feature)\n",
    "                if keyword in tokens:\n",
    "                    adjectives = getAdjectives(tweet)  # This will get all the adjectives, not just one\n",
    "                    features = []\n",
    "                    for adjective in adjectives:\n",
    "                        adjective = re.sub('-', '_',\n",
    "                                           adjective)  \n",
    "                        features.append(keyword + '_' + adjective)\n",
    "                    feature = ' '.join(features)\n",
    "                    targets_feature.append(feature)\n",
    "\n",
    "            tweet_target_features.append(' '.join(targets_feature))\n",
    "        return tweet_target_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon - Hu and Liu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006 4783\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Get the positiv - negative word lexicon\n",
    "#\n",
    "positive_lexicon_path = '/Users/gautamborgohain/PycharmProjects/DT_Labs/PLayground/Gautam_Borg/HuLiuLexicon/positive-words.txt'\n",
    "negative_lexicon_path ='/Users/gautamborgohain/PycharmProjects/DT_Labs/PLayground/Gautam_Borg/HuLiuLexicon/negative-words.txt'\n",
    "poshand = open(positive_lexicon_path)\n",
    "neghand = open(negative_lexicon_path)\n",
    "poslist = []\n",
    "neglist = []\n",
    "for line in poshand:\n",
    "    poslist.append(re.sub(r'\\n','',line))\n",
    "for line in neghand:\n",
    "    neglist.append(re.sub(r'\\n','',line))\n",
    "print(len(poslist),len(neglist))\n",
    "\n",
    "poshand.close()\n",
    "neghand.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPositiveWordCount(tweet):\n",
    "    countPos = 0\n",
    "    for word in word_tokenize(tweet):\n",
    "        if len(word)>=2 and word in poslist: countPos+=1\n",
    "    return countPos\n",
    "\n",
    "def getNegativeWordCount(tweet):\n",
    "    countNeg = 0\n",
    "    for word in word_tokenize(tweet):\n",
    "        if len(word)>=2 and word in neglist: countNeg+=1\n",
    "    return countNeg\n",
    "\n",
    "class lexiconSent(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform(self, frame):\n",
    "        df = pd.DataFrame(columns=['POS_LEX','NEG_LEX'])\n",
    "        df['POS_LEX'] =   [getPositiveWordCount(tweet) for tweet in frame]\n",
    "        df['NEG_LEX'] =   [getNegativeWordCount(tweet) for tweet in frame]\n",
    "             \n",
    "        return df.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class punctuations(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, frame):\n",
    "        df = pd.DataFrame()\n",
    "        df['PUNC_EXCL'] = [len(re.findall(r'!',tweet)) for tweet in frame]\n",
    "        df['PUNC_QUES'] = [len(re.findall(r'\\?',tweet)) for tweet in frame]\n",
    "        return df.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data_unlab_copy['EMOT_HAPPY'] = [len(re.findall(r':-D|:D|:-\\)|:\\)',tweet)) for tweet in data_unlab_copy['Tweet']]\n",
    "# data_unlab_copy['EMOT_SAD'] = [len(re.findall(r':-\\(|:\\(',tweet)) for tweet in data_unlab_copy['Tweet']]\n",
    "# data_unlab_copy['EMOT_WINK'] = [len(re.findall(r';-\\)|;\\)',tweet)) for tweet in data_unlab_copy['Tweet']]\n",
    "# data_unlab_copy['ANGRY_EMO'] = [len(re.findall(r'\\U0001F621|\\U0001F624|\\U0001F63E|\\U0001F449|\\U0001F44A',tweet)) for tweet in data_unlab_copy['Tweet']]\n",
    "# data_unlab_copy['LOVE_EMO'] = [len(re.findall(r'\\U0001F618|\\U0001F61A|\\U0001F63B|\\U0001F63D',tweet)) for tweet in data_unlab_copy['Tweet']]\n",
    "# data_unlab_copy['HAPPY_EMO'] = [len(re.findall(r'\\U0001F602|\\U0001F603|\\U0001F604|\\U0001F605|\\U0001F606|\\U0001F609|\\U0001F60A|\\U0001F60B|\\U0001F60C|\\U0001F60D|\\U0001F60F|\\U0001F612|\\U0001F61C|\\U0001F61D|\\U0001F638|\\U0001F639|\\U0001F63A|\\U0001F63C|\\U0001F44C|\\U0001F44D|\\U0001F44F|\\U0001F450|\\U0001F451|\\U0001F600|\\U0001F607|\\U0001F608|\\U0001F60E|\\U0001F617|\\U0001F619|\\U0001F61B|\\U0001f917|\\U0001f595|\\U0001f389|\\U0001f38a',tweet)) for tweet in data_unlab_copy['Tweet']]\n",
    "# data_unlab_copy['NEUTRAL_EMO'] = [len(re.findall(r'U0001F601|\\U0001F633|\\U0001F645|\\U0001F646|\\U0001F647|\\U0001F648|\\U0001F649|\\U0001F64A|\\U0001F64B|\\U0001F64C|\\U0001F64D|\\U0001F64E|\\U0001F64F|\\U0001F448|\\U0001F44B|\\U0001F610|\\U0001F611|\\U0001F615|\\U0001F62C|\\U0001F636',tweet)) for tweet in data_unlab_copy['Tweet']]\n",
    "# data_unlab_copy['SAD_EMO'] = [len(re.findall(r'\\U0001F613|\\U0001F614|\\U0001F616|\\U0001F61E|\\U0001F620|\\U0001F622|\\U0001F623|\\U0001F625|\\U0001F628|\\U0001F629|\\U0001F62A|\\U0001F62B|\\U0001F62D|\\U0001F630|\\U0001F631|\\U0001F632|\\U0001F635|\\U0001F637|\\U0001F63F|\\U0001F640|\\U0001F44E|\\U0001f634|\\U0001F61F|\\U0001F626|\\U0001F627|\\U0001F62E|\\U0001F62F|\\U0001F634',tweet)) for tweet in data_unlab_copy['Tweet']]\n",
    "\n",
    "class emoticons(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, frame):\n",
    "        df = pd.DataFrame()\n",
    "        posEmot,negEmot,posEmo,negEmo,neutralEmo = [],[],[],[],[]\n",
    "        for tweet in frame:\n",
    "            posEmot.append(len(re.findall(r':-D|:D|:-\\)|:\\)|;-\\)|;\\)',tweet)))\n",
    "            negEmot.append(len(re.findall(r':-\\(|:\\(',tweet)))\n",
    "            posEmo.append(len(re.findall(r'\\U0001F618|\\U0001F61A|\\U0001F63B|\\U0001F63D|\\U0001F602|\\U0001F603|\\U0001F604|\\U0001F605|\\U0001F606|\\U0001F609|\\U0001F60A|\\U0001F60B|\\U0001F60C|\\U0001F60D|\\U0001F60F|\\U0001F612|\\U0001F61C|\\U0001F61D|\\U0001F638|\\U0001F639|\\U0001F63A|\\U0001F63C|\\U0001F44C|\\U0001F44D|\\U0001F44F|\\U0001F450|\\U0001F451|\\U0001F600|\\U0001F607|\\U0001F608|\\U0001F60E|\\U0001F617|\\U0001F619|\\U0001F61B|\\U0001f917|\\U0001f595|\\U0001f389|\\U0001f38a',tweet)))\n",
    "            negEmo.append(len(re.findall(r'\\U0001F621|\\U0001F624|\\U0001F63E|\\U0001F449|\\U0001F44A|\\U0001F613|\\U0001F614|\\U0001F616|\\U0001F61E|\\U0001F620|\\U0001F622|\\U0001F623|\\U0001F625|\\U0001F628|\\U0001F629|\\U0001F62A|\\U0001F62B|\\U0001F62D|\\U0001F630|\\U0001F631|\\U0001F632|\\U0001F635|\\U0001F637|\\U0001F63F|\\U0001F640|\\U0001F44E|\\U0001f634|\\U0001F61F|\\U0001F626|\\U0001F627|\\U0001F62E|\\U0001F62F|\\U0001F634',tweet)) )\n",
    "            neutralEmo.append(len(re.findall(r'U0001F601|\\U0001F633|\\U0001F645|\\U0001F646|\\U0001F647|\\U0001F648|\\U0001F649|\\U0001F64A|\\U0001F64B|\\U0001F64C|\\U0001F64D|\\U0001F64E|\\U0001F64F|\\U0001F448|\\U0001F44B|\\U0001F610|\\U0001F611|\\U0001F615|\\U0001F62C|\\U0001F636',tweet)) )\n",
    "            \n",
    "        df['EMOT_POS'] = posEmot\n",
    "        df['EMOT_NEG'] = negEmot\n",
    "        df['EMO_POS'] = posEmo\n",
    "        df['EMO_NEG'] = negEmo\n",
    "        df['EMO_NEUTRAL'] = neutralEmo\n",
    "        \n",
    "        return df.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type Dependency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Stanford Core NLP dependency tree features\n",
    "\n",
    "cd stanford-corenlp-full-2015-12-09/\n",
    "\n",
    "export CLASSPATH=\"`find . -name '*.jar'`\" \n",
    "\n",
    "java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'dep': 'ROOT', 'governorGloss': 'ROOT', 'dependentGloss': 'guys', 'governor': 0, 'dependent': 2}, {'dep': 'compound', 'governorGloss': 'guys', 'dependentGloss': 'U', 'governor': 2, 'dependent': 1}, {'dep': 'amod', 'governorGloss': 'wake', 'dependentGloss': 'better', 'governor': 4, 'dependent': 3}, {'dep': 'dep', 'governorGloss': 'guys', 'dependentGloss': 'wake', 'governor': 2, 'dependent': 4}, {'dep': 'case', 'governorGloss': 'idea', 'dependentGloss': 'up', 'governor': 7, 'dependent': 5}, {'dep': 'compound', 'governorGloss': 'idea', 'dependentGloss': 'ur', 'governor': 7, 'dependent': 6}, {'dep': 'nmod', 'governorGloss': 'wake', 'dependentGloss': 'idea', 'governor': 4, 'dependent': 7}, {'dep': 'punct', 'governorGloss': 'guys', 'dependentGloss': '!', 'governor': 2, 'dependent': 8}]\n"
     ]
    }
   ],
   "source": [
    "from pycorenlp import StanfordCoreNLP\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "# Sample output from the Stanford dependency parser\n",
    "\n",
    "text = (\"U guys better wake up ur idea! People pay so much for ur crappy service. People late for work now. ATSMRT\")\n",
    "output = nlp.annotate(text, properties={\n",
    "        'annotators': 'parse,relation',\n",
    "        'outputFormat': 'json'\n",
    "    })\n",
    "# print(output)\n",
    "print(output['sentences'][0]['basic-dependencies'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def posorneg(word):\n",
    "    if(len(poslist)>0 and len(neglist)>0):\n",
    "        word = word.lower()\n",
    "        if word in poslist:\n",
    "            return 'POSITIVE'\n",
    "        elif word in neglist:\n",
    "            return 'NEGATIVE'\n",
    "        else: return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targets = ['SMRT','smrt','smrt_singapore','SMRT_SINGAPORE','train','mrt','TARGET','people','LRT','lrt']\n",
    "verbs = ['VBG','VB','VBD','VBN','VBP','VBZ']\n",
    "adjnon = ['JJ','JJR','JJS','NN','NNS','NNP','NNPS']\n",
    "objects = ['obj','pobj','dobj','iobj','nmod']\n",
    "adverbs = ['RB','RBR','RBS']\n",
    "subjs = ['nsubj','dep']\n",
    "\n",
    "#helper functions\n",
    "def getDependentGloss(dep,word, depToSearch):\n",
    "    return dep.get('dependentGloss') if dep.get('governorGloss') == word and dep.get('dep') in depToSearch else ''\n",
    "\n",
    "def getDependentGloss_WithTargets(dep,word):\n",
    "    return dep.get('dependentGloss') if dep.get('governorGloss') == word and dep.get('dependentGloss') in targets else ''\n",
    "\n",
    "def isTransitive(verb,depdicts):\n",
    "    for dep in depdicts:\n",
    "        if dep.get('governorGloss')  == verb and dep.get('dep') in objects:\n",
    "            return True        \n",
    "    return False\n",
    "\n",
    "class typeDependency(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, frame):\n",
    "        dependecyfeatures = []\n",
    "        for tweet in frame:\n",
    "            tweet = tweet.encode('utf-8')\n",
    "            output = nlp.annotate(tweet, properties={\n",
    "                    'annotators': 'parse,relation',\n",
    "                    'outputFormat': 'json'\n",
    "                })\n",
    "            features = []\n",
    "            for i in range(len(output['sentences'])):\n",
    "                posdicts = output['sentences'][i]['tokens']\n",
    "                depdicts = output['sentences'][i]['basic-dependencies']\n",
    "                count = 0\n",
    "                for pos in posdicts:\n",
    "                    verb = pos.get('word') if pos.get('pos') in verbs else '' # Get Verbs\n",
    "                    if verb != '':\n",
    "                        if(isTransitive(verb, depdicts)):\n",
    "                            for dep in depdicts:                  \n",
    "                                #Rule 1\n",
    "                                dependent = getDependentGloss(dep,verb,objects)\n",
    "                                if dependent != '' and dependent in targets:\n",
    "                                    features.append(posorneg(verb)+'_arg2')\n",
    "                                #Rule 2\n",
    "                                dependent = getDependentGloss(dep,verb,subjs)\n",
    "                                if (dependent != '' and dependent in targets):\n",
    "                                    features.append(posorneg(verb)+'_arg1')\n",
    "                        else:\n",
    "                            #Rule 3\n",
    "                            for dep in depdicts:\n",
    "                                dependent = getDependentGloss(dep,verb,subjs)\n",
    "                                if dependent != '' and dependent in targets:\n",
    "                                    features.append(posorneg(verb)+'_it_arg1')\n",
    "                    #Rule 4            \n",
    "                    adj_noun = pos.get('word') if pos.get('pos') in adjnon else '' # Get Adjectives and Nouns\n",
    "                    if adj_noun != '':\n",
    "                        for dep in depdicts:\n",
    "                            dependent = getDependentGloss_WithTargets(dep,adj_noun)\n",
    "                            if dependent != '':features.append(posorneg(adj_noun)+'_arg1')\n",
    "                    #Rule 7\n",
    "                    adv = pos.get('word') if pos.get('pos') in adverbs else ''\n",
    "                    if adv != '':    \n",
    "                        for dep in depdicts:\n",
    "                            verb = dep.get('governorGloss') if dep.get('dependentGloss') == adv else ''# Get the verb it modifies\n",
    "                            if verb != '':\n",
    "                                for dep1 in depdicts:#Loop again and check the target\n",
    "                                    dependent = getDependentGloss_WithTargets(dep1,verb)\n",
    "                                    if dependent != '':\n",
    "                                        # Rule 8 - negation \n",
    "                                        if dep.get('dep') == 'neg':\n",
    "                                            features.append('arg1_v_neg_'+posorneg(verb))\n",
    "                                        else:\n",
    "                                            features.append('arg1_v_'+posorneg(adv))\n",
    "                #Rule 5\n",
    "                for dep in depdicts:\n",
    "                    dependent = dep.get('dependentGloss') if i>0 and dep.get('governorGloss') =='ROOT' and len(features)>0 else ''\n",
    "                    if dependent != '' : \n",
    "                        for pos in posdicts:\n",
    "                            if pos.get('word') == dependent and pos.get('pos') in adjnon:\n",
    "                                features.append(posorneg(dependent)+'_arg')\n",
    "                    # If in the second sentence,for the root, theres is already a target dependent feature in the list \n",
    "                    #  suggesting that that target appears in the previous snentence  and it is an adjective or noun\n",
    "\n",
    "\n",
    "            dependecyfeatures.append(' '.join(features))\n",
    "\n",
    "        return dependecyfeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "\n",
    "Bring all the work together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Temp for testing the pipeline on jsut 10 records\n",
    "Xtemp = X[0:10]\n",
    "Ytemp = y[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2374\n"
     ]
    }
   ],
   "source": [
    "perc = 0.6\n",
    "training_set = data.sample(frac = perc, random_state=0)\n",
    "testing_set = data.loc[~data.index.isin(training_set.index)]\n",
    "ytrain = training_set.Sentiment\n",
    "Xtrain = training_set.Tweet\n",
    "print(len(Xtrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1583\n"
     ]
    }
   ],
   "source": [
    "Xtest = testing_set.Tweet\n",
    "ytest = testing_set.Sentiment\n",
    "print(len(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('regexProcess', regExProcesses()), ('features', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('bow', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=500, max_featur...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('regexProcess', regExProcesses()),\n",
    "    ('features', FeatureUnion([\n",
    "                    ('bow', CountVectorizer(min_df=4,max_df = 500,stop_words='english',lowercase=True,ngram_range=(1,4))),\n",
    "                    ('lex',lexiconSent()),\n",
    "                    ('punc',punctuations()),\n",
    "                    ('emot',emoticons()),\n",
    "                    ('pos', Pipeline([\n",
    "                                ('pos',posTagTweets()),\n",
    "                                ('pos_vect', CountVectorizer())\n",
    "                            ])),\n",
    "                    ('subj',Pipeline([\n",
    "                                ('subj',subjectivityLexicon()),\n",
    "                                ('subj_vect', CountVectorizer())\n",
    "                            ])),\n",
    "                    ('targ',Pipeline([\n",
    "                                ('targ',enhancedTargetFeats()),\n",
    "                                ('targ_vect', CountVectorizer())\n",
    "                            ])),\n",
    "                    ('typd',Pipeline([\n",
    "                                ('typd',typeDependency()),\n",
    "                                ('typd_vect', CountVectorizer())\n",
    "                            ]))\n",
    "                ])\n",
    "    ),\n",
    "    ('svm', LinearSVC())    \n",
    "    ])\n",
    "\n",
    "# clf = pipeline.fit(Xtemp,Ytemp)\n",
    "\n",
    "pipeline.fit(Xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_train = pipeline.transform(Xtemp)\n",
    "temp_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_train = pipeline.transform(temp)\n",
    "test_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98399326032013479"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.score(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83891345546430829"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = pipeline.predict(Xtest)\n",
    "accuracy_score(predictions,ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline accuracy of the datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.960404380792\n",
      "0.834491471889\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('regexProcess', regExProcesses()),\n",
    "    ('features', FeatureUnion([\n",
    "                    ('bow', CountVectorizer(min_df=4,max_df = 500,stop_words='english',lowercase=True,ngram_range=(1,4))),\n",
    "                    ('lex',lexiconSent()),\n",
    "                    ('punc',punctuations()),\n",
    "                    ('emot',emoticons())\n",
    "                ])\n",
    "    ),\n",
    "    ('svm', LinearSVC())    \n",
    "    ])\n",
    "\n",
    "# clf = pipeline.fit(Xtemp,Ytemp)\n",
    "\n",
    "pipeline.fit(Xtrain,ytrain)\n",
    "print(pipeline.score(Xtrain,ytrain))\n",
    "predictions = pipeline.predict(Xtest)\n",
    "print(accuracy_score(predictions, ytest)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using only the BOW features, the accuracy on the test set was 81% and with the other features (as seen in the pipeline),\n",
    "the accuracy increased to 83.4%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluateModel(X,y,clf):\n",
    "    \n",
    "    predictions_training = clf.predict(X)\n",
    "#     Calculate the accuracy\n",
    "    accuracy_training = accuracy_score(y, predictions_training)\n",
    "#     cv_scores = cross_validation.cross_val_score(clf, X, y, cv=10)\n",
    "    #Print the accuracy, cross validation scores and the crosstab of the predictions \n",
    "    print(\"Accuracy on the training data : \", accuracy_training)\n",
    "#     print(\"Cross Validation Accuracy scores - \",cv_scores)\n",
    "#     print(\"Cross Validation Accuracy - Training set: %0.2f (+/- %0.2f)\" % (cv_scores.mean(), cv_scores.std() * 2))\n",
    "    print(\"Training data Crosstab: \\n\", pd.crosstab(y, predictions_training))\n",
    "\n",
    "    # ROC Curve\n",
    "    y_score = clf.decision_function(X)\n",
    "    target_testting_dummies = pd.get_dummies(y)\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(0,len(np.unique(y))):\n",
    "        fpr[i], tpr[i], _ = roc_curve(target_testting_dummies[target_testting_dummies.columns[i]], y_score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr[2], tpr[2], label='Positive ROC curve (area = %0.2f)' % roc_auc[2])\n",
    "    plt.plot(fpr[0], tpr[0], label='Negative ROC curve (area = %0.2f)' % roc_auc[0])\n",
    "    plt.plot(fpr[1], tpr[1], label='Neutral ROC curve (area = %0.2f)' % roc_auc[1])\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluateModel(Xtemp,Ytemp,pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to pickle and load it back to test on different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TwitterSentiment_SMRT_model_All_60.pkl',\n",
       " 'TwitterSentiment_SMRT_model_All_60.pkl_01.npy',\n",
       " 'TwitterSentiment_SMRT_model_All_60.pkl_02.npy',\n",
       " 'TwitterSentiment_SMRT_model_All_60.pkl_03.npy']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(pipeline,'TwitterSentiment_SMRT_model_All_60.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "pipe = joblib.load('TwitterSentiment_SMRT_model_All_60.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pipe.predict(['If you are in Vancouver this weekend, check out @staticstars on Sat. at 20:00 @ The Commo in Vancouver,'])\n",
    "\n",
    "pipe.predict(Xtemp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Sem Eval dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/pandas/core/generic.py:4059: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n",
      "/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/IPython/core/interactiveshell.py:3066: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       " 0    3548\n",
       " 1    2970\n",
       "-1    1331\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('/Users/gautamborgohain/Google Drive/CI/twitter_download20160410.xlsx')\n",
    "cleaned = df[df.Tweet != 'Not Available']\n",
    "cleaned.Sentiment[cleaned.Sentiment == 'neutral'] = 0\n",
    "cleaned.Sentiment[cleaned.Sentiment == 'positive'] = 1\n",
    "cleaned.Sentiment[cleaned.Sentiment == 'negative'] = -1\n",
    "cleaned.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "pipe = joblib.load('TwitterSentiment_SMRT_model_All_60.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7849"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = cleaned.Tweet\n",
    "y = cleaned.Sentiment\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SE_predicted = pipe.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_native = [int(pred) for pred in y]\n",
    "type(np.unique(y_native)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(np.unique(SE_predicted)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4181424385272009"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(SE_predicted,y_native)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very low accuracy because \n",
    "- The training of the model was done on a different dataset and the testing was done on a dataset that was of entirely a differnt domain.\n",
    "- The data set that the model was trained on was labelled in a target dependent manner\n",
    "- The model takes into account a static number of targets which is used for generating the target dependent features, which would not be available in this dataset that has nothing to do with SMRT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem of the dataset being not labelled properly can also be seen here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = ['I do not like SMRT', 'SMRT is just not bad']\n",
    "pipeline.predict(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline to train a model using the SEm Eval data set, without any target dependent features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4709\n",
      "3140\n"
     ]
    }
   ],
   "source": [
    "perc = 0.6\n",
    "training_set = cleaned.sample(frac = perc, random_state=0)\n",
    "testing_set = cleaned.loc[~cleaned.index.isin(training_set.index)]\n",
    "ytrain = training_set.Sentiment\n",
    "ytrain = [int(label) for label in ytrain]\n",
    "Xtrain = training_set.Tweet\n",
    "ytest = testing_set.Sentiment\n",
    "ytest = [int(label) for label in ytest]\n",
    "Xtest = testing_set.Tweet\n",
    "print(len(Xtrain))\n",
    "print(len(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('regexProcess', regExProcesses()), ('features', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('bow', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=500, max_featur...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('regexProcess', regExProcesses()),\n",
    "    ('features', FeatureUnion([\n",
    "                    ('bow', CountVectorizer(min_df=4,max_df = 500,stop_words='english',lowercase=True,ngram_range=(1,4))),\n",
    "                    ('lex',lexiconSent()),\n",
    "                    ('punc',punctuations()),\n",
    "                    ('emot',emoticons()),\n",
    "                    ('pos', Pipeline([\n",
    "                                ('pos',posTagTweets()),\n",
    "                                ('pos_vect', CountVectorizer())\n",
    "                            ])),\n",
    "                    ('subj',Pipeline([\n",
    "                                ('subj',subjectivityLexicon()),\n",
    "                                ('subj_vect', CountVectorizer())\n",
    "                            ])),\n",
    "#                     ('targ',Pipeline([\n",
    "#                                 ('targ',enhancedTargetFeats()),\n",
    "#                                 ('targ_vect', CountVectorizer())\n",
    "#                             ])),\n",
    "#                     ('typd',Pipeline([\n",
    "#                                 ('typd',typeDependency()),\n",
    "#                                 ('typd_vect', CountVectorizer())\n",
    "#                             ]))\n",
    "                ])\n",
    "    ),\n",
    "    ('svm', LinearSVC())    \n",
    "    ])\n",
    "\n",
    "# clf = pipeline.fit(Xtemp,Ytemp)\n",
    "\n",
    "pipeline.fit(Xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95986409004\n",
      "0.644585987261\n"
     ]
    }
   ],
   "source": [
    "print(pipeline.score(Xtrain,ytrain))\n",
    "predictions  = pipeline.predict(Xtest)\n",
    "print(accuracy_score(predictions, ytest)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.400505369551\n"
     ]
    }
   ],
   "source": [
    "## Applyin gthis on our SMRT dataset\n",
    "# we got 83% accuracy on this using the model that we trained using the target dependent features and the data the we labeled\n",
    "\n",
    "predictions  = pipeline.predict(Xtest)\n",
    "print(accuracy_score(predictions, ytest)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.939052877469\n",
      "0.617515923567\n"
     ]
    }
   ],
   "source": [
    "# Tryin another one with just the BOW features to get the baseline accuracy and see if there is incerease in accuracy\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('regexProcess', regExProcesses()),\n",
    "    ('features', FeatureUnion([\n",
    "                    ('bow', CountVectorizer(min_df=4,max_df = 500,stop_words='english',lowercase=True,ngram_range=(1,4))),\n",
    "#                     ('lex',lexiconSent()),\n",
    "#                     ('punc',punctuations()),\n",
    "#                     ('emot',emoticons()),\n",
    "#                     ('pos', Pipeline([\n",
    "#                                 ('pos',posTagTweets()),\n",
    "#                                 ('pos_vect', CountVectorizer())\n",
    "#                             ])),\n",
    "#                     ('subj',Pipeline([\n",
    "#                                 ('subj',subjectivityLexicon()),\n",
    "#                                 ('subj_vect', CountVectorizer())\n",
    "#                             ])),\n",
    "#                     ('targ',Pipeline([\n",
    "#                                 ('targ',enhancedTargetFeats()),\n",
    "#                                 ('targ_vect', CountVectorizer())\n",
    "#                             ])),\n",
    "#                     ('typd',Pipeline([\n",
    "#                                 ('typd',typeDependency()),\n",
    "#                                 ('typd_vect', CountVectorizer())\n",
    "#                             ]))\n",
    "                ])\n",
    "    ),\n",
    "    ('svm', LinearSVC())    \n",
    "    ])\n",
    "\n",
    "# clf = pipeline.fit(Xtemp,Ytemp)\n",
    "\n",
    "pipeline.fit(Xtrain,ytrain)\n",
    "print(pipeline.score(Xtrain,ytrain))\n",
    "predictions  = pipeline.predict(Xtest)\n",
    "print(accuracy_score(predictions, ytest)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the baseline accuracy of this dataset is 61% !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# With the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Created At</th>\n",
       "      <th>DayWeek</th>\n",
       "      <th>DayYear</th>\n",
       "      <th>Hour</th>\n",
       "      <th>ID</th>\n",
       "      <th>In Reply To</th>\n",
       "      <th>Language</th>\n",
       "      <th>Place</th>\n",
       "      <th>ReTweet Count</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>User Handle</th>\n",
       "      <th>isSwarm</th>\n",
       "      <th>POSITVE_LEX</th>\n",
       "      <th>NEGATIVE_LEX</th>\n",
       "      <th>PUNC_EXCL</th>\n",
       "      <th>PUNC_QUES</th>\n",
       "      <th>EMOT_HAPPY</th>\n",
       "      <th>EMOT_SAD</th>\n",
       "      <th>EMOT_WINK</th>\n",
       "      <th>ANGRY_EMO</th>\n",
       "      <th>LOVE_EMO</th>\n",
       "      <th>HAPPY_EMO</th>\n",
       "      <th>NEUTRAL_EMO</th>\n",
       "      <th>SAD_EMO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-03-23 07:58:09</td>\n",
       "      <td>2</td>\n",
       "      <td>83</td>\n",
       "      <td>7</td>\n",
       "      <td>712428180989042048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>RT NOT_TARGET TODAY'S TOP STORY: HASH_MRT trai...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vxnnnn</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-03-23 07:55:48</td>\n",
       "      <td>2</td>\n",
       "      <td>83</td>\n",
       "      <td>7</td>\n",
       "      <td>712427591685136000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>On Facebook, condolences pour forth for 2 SMRT...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ThinaeshS</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-03-23 07:53:36</td>\n",
       "      <td>2</td>\n",
       "      <td>83</td>\n",
       "      <td>7</td>\n",
       "      <td>712427035713404032</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>289</td>\n",
       "      <td>0</td>\n",
       "      <td>RT NOT_TARGET SMRT's fatal accident near HASH_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LforLana</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-03-23 07:46:04</td>\n",
       "      <td>2</td>\n",
       "      <td>83</td>\n",
       "      <td>7</td>\n",
       "      <td>712425139594328064</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>181</td>\n",
       "      <td>0</td>\n",
       "      <td>RT NOT_TARGET JUST IN: SMRT releases 2 photos ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vlxhh</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-03-23 07:45:34</td>\n",
       "      <td>2</td>\n",
       "      <td>83</td>\n",
       "      <td>7</td>\n",
       "      <td>712425016323778048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>RT NOT_TARGET HAPPENING TODAY: HASH_LKY’s 1st ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adorenew</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Created At  DayWeek  DayYear  Hour                  ID In Reply To  \\\n",
       "0 2016-03-23 07:58:09        2       83     7  712428180989042048         NaN   \n",
       "1 2016-03-23 07:55:48        2       83     7  712427591685136000         NaN   \n",
       "2 2016-03-23 07:53:36        2       83     7  712427035713404032         NaN   \n",
       "3 2016-03-23 07:46:04        2       83     7  712425139594328064         NaN   \n",
       "4 2016-03-23 07:45:34        2       83     7  712425016323778048         NaN   \n",
       "\n",
       "  Language Place  ReTweet Count  Sentiment  \\\n",
       "0       en   NaN             11          0   \n",
       "1       en   NaN              0          0   \n",
       "2       en   NaN            289          0   \n",
       "3       en   NaN            181          0   \n",
       "4       en   NaN             10          0   \n",
       "\n",
       "                                               Tweet Unnamed: 2 User Handle  \\\n",
       "0  RT NOT_TARGET TODAY'S TOP STORY: HASH_MRT trai...        NaN      vxnnnn   \n",
       "1  On Facebook, condolences pour forth for 2 SMRT...        NaN   ThinaeshS   \n",
       "2  RT NOT_TARGET SMRT's fatal accident near HASH_...        NaN    LforLana   \n",
       "3  RT NOT_TARGET JUST IN: SMRT releases 2 photos ...        NaN       vlxhh   \n",
       "4  RT NOT_TARGET HAPPENING TODAY: HASH_LKY’s 1st ...        NaN    adorenew   \n",
       "\n",
       "  isSwarm  POSITVE_LEX  NEGATIVE_LEX  PUNC_EXCL  PUNC_QUES  EMOT_HAPPY  \\\n",
       "0   False            0             2          0          0           0   \n",
       "1   False            0             1          0          0           0   \n",
       "2   False            0             1          0          0           0   \n",
       "3   False            0             1          0          0           0   \n",
       "4   False            0             2          0          0           0   \n",
       "\n",
       "   EMOT_SAD  EMOT_WINK  ANGRY_EMO  LOVE_EMO  HAPPY_EMO  NEUTRAL_EMO  SAD_EMO  \n",
       "0         0          0          0         0          0            0        0  \n",
       "1         0          0          0         0          0            0        0  \n",
       "2         0          0          0         0          0            0        0  \n",
       "3         0          0          0         0          0            0        0  \n",
       "4         0          0          0         0          0            0        0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel('/Users/gautamborgohain/Desktop/CI_Data_Labelled2.xlsx')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0    1841\n",
       "-1     733\n",
       " 1     275\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1709\n",
      "1140\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_excel('/Users/gautamborgohain/Desktop/CI_Data_Labelled2.xlsx')\n",
    "data.index = np.arange(len(data))\n",
    "\n",
    "perc = 0.6\n",
    "training_set = data.sample(frac = perc, random_state=0)\n",
    "testing_set = data.loc[~data.index.isin(training_set.index)]\n",
    "ytrain = training_set.Sentiment\n",
    "Xtrain = training_set.Tweet\n",
    "print(len(Xtrain))\n",
    "Xtest = testing_set.Tweet\n",
    "ytest = testing_set.Sentiment\n",
    "print(len(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.953188999415\n",
      "0.744736842105\n"
     ]
    }
   ],
   "source": [
    "#The baseline score using only the BOW\n",
    "pipeline = Pipeline([\n",
    "    ('regexProcess', regExProcesses()),\n",
    "    ('features', FeatureUnion([\n",
    "                    ('bow', CountVectorizer(min_df=4,max_df = 500,stop_words='english',lowercase=True,ngram_range=(1,4))),\n",
    "#                     ('lex',lexiconSent()),\n",
    "#                     ('punc',punctuations()),\n",
    "#                     ('emot',emoticons()),\n",
    "#                     ('pos', Pipeline([\n",
    "#                                 ('pos',posTagTweets()),\n",
    "#                                 ('pos_vect', CountVectorizer())\n",
    "#                             ])),\n",
    "#                     ('subj',Pipeline([\n",
    "#                                 ('subj',subjectivityLexicon()),\n",
    "#                                 ('subj_vect', CountVectorizer())\n",
    "#                             ])),\n",
    "#                     ('targ',Pipeline([\n",
    "#                                 ('targ',enhancedTargetFeats()),\n",
    "#                                 ('targ_vect', CountVectorizer())\n",
    "#                             ])),\n",
    "#                     ('typd',Pipeline([\n",
    "#                                 ('typd',typeDependency()),\n",
    "#                                 ('typd_vect', CountVectorizer())\n",
    "#                             ]))\n",
    "                ])\n",
    "    ),\n",
    "    ('svm', LinearSVC())    \n",
    "    ])\n",
    "\n",
    "# clf = pipeline.fit(Xtemp,Ytemp)\n",
    "\n",
    "pipeline.fit(Xtrain,ytrain)\n",
    "\n",
    "print(pipeline.score(Xtrain,ytrain))\n",
    "predictions  = pipeline.predict(Xtest)\n",
    "print(accuracy_score(predictions, ytest)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.959040374488\n",
      "0.765789473684\n"
     ]
    }
   ],
   "source": [
    "#The baseline score using no target dependent features\n",
    "pipeline = Pipeline([\n",
    "    ('regexProcess', regExProcesses()),\n",
    "    ('features', FeatureUnion([\n",
    "                    ('bow', CountVectorizer(min_df=4,max_df = 500,stop_words='english',lowercase=True,ngram_range=(1,4))),\n",
    "                    ('lex',lexiconSent()),\n",
    "                    ('punc',punctuations()),\n",
    "                    ('emot',emoticons()),\n",
    "#                     ('pos', Pipeline([\n",
    "#                                 ('pos',posTagTweets()),\n",
    "#                                 ('pos_vect', CountVectorizer())\n",
    "#                             ])),\n",
    "#                     ('subj',Pipeline([\n",
    "#                                 ('subj',subjectivityLexicon()),\n",
    "#                                 ('subj_vect', CountVectorizer())\n",
    "#                             ])),\n",
    "#                     ('targ',Pipeline([\n",
    "#                                 ('targ',enhancedTargetFeats()),\n",
    "#                                 ('targ_vect', CountVectorizer())\n",
    "#                             ])),\n",
    "#                     ('typd',Pipeline([\n",
    "#                                 ('typd',typeDependency()),\n",
    "#                                 ('typd_vect', CountVectorizer())\n",
    "#                             ]))\n",
    "                ])\n",
    "    ),\n",
    "    ('svm', LinearSVC())    \n",
    "    ])\n",
    "\n",
    "# clf = pipeline.fit(Xtemp,Ytemp)\n",
    "\n",
    "pipeline.fit(Xtrain,ytrain)\n",
    "\n",
    "print(pipeline.score(Xtrain,ytrain))\n",
    "predictions  = pipeline.predict(Xtest)\n",
    "print(accuracy_score(predictions, ytest)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance does increase as we add the different chuncks of features into the model.However we need to do a T-test to see if it the idfference is statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99122293739\n",
      "0.771929824561\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('regexProcess', regExProcesses()),\n",
    "    ('features', FeatureUnion([\n",
    "                    ('bow', CountVectorizer(min_df=4,max_df = 500,stop_words='english',lowercase=True,ngram_range=(1,4))),\n",
    "                    ('lex',lexiconSent()),\n",
    "                    ('punc',punctuations()),\n",
    "                    ('emot',emoticons()),\n",
    "                    ('pos', Pipeline([\n",
    "                                ('pos',posTagTweets()),\n",
    "                                ('pos_vect', CountVectorizer())\n",
    "                            ])),\n",
    "                    ('subj',Pipeline([\n",
    "                                ('subj',subjectivityLexicon()),\n",
    "                                ('subj_vect', CountVectorizer())\n",
    "                            ])),\n",
    "                    ('targ',Pipeline([\n",
    "                                ('targ',enhancedTargetFeats()),\n",
    "                                ('targ_vect', CountVectorizer())\n",
    "                            ])),\n",
    "                    ('typd',Pipeline([\n",
    "                                ('typd',typeDependency()),\n",
    "                                ('typd_vect', CountVectorizer())\n",
    "                            ]))\n",
    "                ])\n",
    "    ),\n",
    "    ('svm', LinearSVC())    \n",
    "    ])\n",
    "\n",
    "# clf = pipeline.fit(Xtemp,Ytemp)\n",
    "\n",
    "pipeline.fit(Xtrain,ytrain)\n",
    "\n",
    "print(pipeline.score(Xtrain,ytrain))\n",
    "predictions  = pipeline.predict(Xtest)\n",
    "print(accuracy_score(predictions, ytest)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TwitterSentiment_NEW_SMRT_model_All_60.pkl',\n",
       " 'TwitterSentiment_NEW_SMRT_model_All_60.pkl_01.npy',\n",
       " 'TwitterSentiment_NEW_SMRT_model_All_60.pkl_02.npy',\n",
       " 'TwitterSentiment_NEW_SMRT_model_All_60.pkl_03.npy']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(pipeline,'TwitterSentiment_NEW_SMRT_model_All_60.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now apply this model to the SEM Eval dataset. fingers crossed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't handle mix of multiclass and unknown",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-08b593861c7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpredictions\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mdiffering_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         raise ValueError(\"Can't handle mix of {0} and {1}\"\n\u001b[0;32m---> 82\u001b[0;31m                          \"\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Can't handle mix of multiclass and unknown"
     ]
    }
   ],
   "source": [
    "X = cleaned.Tweet\n",
    "y = cleaned.Sentiment\n",
    "\n",
    "predictions  = pipeline.predict(X)\n",
    "print(accuracy_score(predictions, y)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.421327557651\n"
     ]
    }
   ],
   "source": [
    "y_native = [int(pred) for pred in y]\n",
    "print(accuracy_score(predictions, y_native)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again very low. But this is expected because we are testing on a different dataset of different domain altogether"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
