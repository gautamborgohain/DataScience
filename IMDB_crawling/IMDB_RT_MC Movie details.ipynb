{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "import socket\n",
    "socket.setdefaulttimeout(5)\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import quote\n",
    "import urllib.request as req\n",
    "\n",
    "user_agent = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'\n",
    "headers={'User-Agent':user_agent,}\n",
    "\n",
    "#\n",
    "# Function to get the soup objects for a given URL\n",
    "#\n",
    "def getSoup(url):\n",
    "    try:\n",
    "        request= req.Request(url,None,headers)\n",
    "        response = req.urlopen(request) \n",
    "        data = response.read()\n",
    "        soup = BeautifulSoup(data, \"html.parser\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"There was an error retrieving\", url, e)  \n",
    "        \n",
    "    finally:\n",
    "        response.close()\n",
    "        \n",
    "    return soup\n",
    "def cleanText(text):\n",
    "    return re.sub(r'\\n','',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched till  0\n",
      "Fetched till  25\n",
      "Fetched till  50\n",
      "Fetched till  75\n",
      "Fetched till  100\n",
      "Fetched till  125\n",
      "Fetched till  150\n",
      "There was an error retrieving http://www.imdb.com/title/tt2082197/ timed out\n",
      "Error at stpe 1 local variable 'response' referenced before assignment\n",
      "Fetched till  175\n",
      "Fetched till  200\n",
      "Fetched till  225\n",
      "Fetched till  250\n",
      "Fetched till  275\n",
      "Fetched till  300\n",
      "Fetched till  325\n",
      "Fetched till  350\n",
      "Fetched till  375\n",
      "Fetched till  400\n",
      "Fetched till  425\n",
      "Fetched till  450\n",
      "Fetched till  475\n",
      "Fetched till  500\n",
      "Fetched till  525\n",
      "Fetched till  550\n",
      "Fetched till  575\n",
      "Fetched till  600\n",
      "Fetched till  625\n",
      "Fetched till  650\n",
      "Fetched till  675\n",
      "Fetched till  700\n",
      "Fetched till  725\n",
      "Fetched till  750\n",
      "Fetched till  775\n",
      "Fetched till  800\n",
      "Fetched till  825\n",
      "Fetched till  850\n",
      "Fetched till  875\n",
      "Fetched till  900\n",
      "Fetched till  925\n",
      "Fetched till  950\n",
      "Fetched till  975\n",
      "Fetched till  0\n",
      "Fetched till  25\n",
      "Fetched till  50\n",
      "Fetched till  75\n",
      "Fetched till  100\n",
      "Fetched till  125\n",
      "Fetched till  150\n",
      "Fetched till  175\n",
      "Fetched till  225\n",
      "Fetched till  250\n",
      "Fetched till  275\n",
      "Fetched till  300\n",
      "Fetched till  325\n",
      "Fetched till  350\n",
      "Fetched till  375\n",
      "Fetched till  400\n",
      "Fetched till  425\n",
      "Fetched till  450\n",
      "Fetched till  475\n",
      "Fetched till  500\n",
      "Fetched till  525\n",
      "Fetched till  550\n",
      "Fetched till  575\n",
      "Fetched till  600\n",
      "Fetched till  625\n",
      "Fetched till  650\n",
      "Fetched till  675\n",
      "Fetched till  700\n",
      "Fetched till  725\n",
      "Fetched till  750\n",
      "Fetched till  775\n",
      "Fetched till  800\n",
      "Fetched till  825\n",
      "Fetched till  850\n",
      "Fetched till  875\n",
      "Fetched till  900\n",
      "Fetched till  925\n",
      "Fetched till  950\n",
      "Fetched till  975\n",
      "Fetched till  0\n",
      "Fetched till  25\n",
      "Fetched till  50\n",
      "Fetched till  75\n",
      "Fetched till  100\n",
      "Fetched till  125\n",
      "Fetched till  150\n",
      "Fetched till  175\n",
      "Fetched till  225\n",
      "Fetched till  250\n",
      "Fetched till  275\n",
      "Fetched till  300\n",
      "Fetched till  325\n",
      "Fetched till  350\n",
      "Fetched till  375\n",
      "Fetched till  400\n",
      "Fetched till  425\n",
      "Fetched till  450\n",
      "Fetched till  475\n",
      "Fetched till  500\n",
      "Fetched till  525\n",
      "Fetched till  550\n",
      "Fetched till  575\n",
      "Fetched till  600\n",
      "Fetched till  625\n",
      "Fetched till  650\n",
      "There was an error retrieving http://www.metacritic.com/search/movie/Frost/Nixon/results HTTP Error 404: Not Found\n",
      "Error at step 3 local variable 'response' referenced before assignment\n",
      "Fetched till  675\n",
      "There was an error retrieving http://www.metacritic.com/search/movie/50/50/results HTTP Error 404: Not Found\n",
      "Error at step 3 local variable 'response' referenced before assignment\n",
      "Fetched till  700\n",
      "Fetched till  725\n",
      "Fetched till  750\n",
      "Fetched till  775\n",
      "Fetched till  800\n",
      "Fetched till  825\n",
      "Fetched till  850\n",
      "Fetched till  875\n",
      "Fetched till  900\n",
      "Fetched till  925\n",
      "Fetched till  950\n",
      "Fetched till  975\n"
     ]
    }
   ],
   "source": [
    "# top250_df_copy = top250_df_copy[0:100]\n",
    "top250_df = pd.DataFrame(columns=['Title','Rating','IMDB_MAIN_URL'])# Initialize an empty dataframe\n",
    "list_home_url = 'http://www.imdb.com/search/title?groups=top_1000&sort=user_rating&start=@@@&view=simple'\n",
    "for i in range(1,1000,100):\n",
    "    pagesoup = getSoup(re.sub('@@@','1',list_home_url))\n",
    "    rows = pagesoup.findAll('tr',{'class':re.compile('even|odd')})\n",
    "    for row in rows[0:10]:\n",
    "        try:\n",
    "            movie_name = row.find('a').text\n",
    "            movie_url = url_start+row.find('a')['href']\n",
    "            rating = row.find('b').text\n",
    "            top250_df.loc[len(top250_df)] = [movie_name,rating,movie_url]\n",
    "        except Exception as e:\n",
    "            print(row.text, movie_url,e)\n",
    "\n",
    "\n",
    "year_list = []\n",
    "all_genre_list = []\n",
    "awards_url_list = []\n",
    "genre_list = []\n",
    "year = 0\n",
    "for index, row in top250_df_copy.iterrows():\n",
    "    try:\n",
    "        temp_url = row['IMDB_MAIN_URL']\n",
    "        oneMovie = getSoup(temp_url)\n",
    "        year_soup = oneMovie.find('span', {'id': 'titleYear'})\n",
    "        year = re.sub(r'[()]', \"\", year_soup.text)\n",
    "        genre_soup = oneMovie.findAll('span', {'itemprop': 'genre'})\n",
    "        genre_list = []\n",
    "        for genre in genre_soup:\n",
    "            genre_list.append(genre.text)\n",
    "        awards_soup = oneMovie.find(text='See more awards')\n",
    "        if (awards_soup):\n",
    "            moreawards_url = url_start + awards_soup.parent['href']\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error at stpe 1\", e)\n",
    "        continue\n",
    "\n",
    "    finally:\n",
    "        year_list.append(year)\n",
    "        all_genre_list.append(' '.join(\n",
    "            genre_list))  # Using a lit here. Maybe put them in different columns? Dont know if that would be useful\n",
    "        awards_url_list.append(moreawards_url)\n",
    "        moreawards_url = \" \"  # Cleaning the variable in case the next movie does not have an awards page\n",
    "        time.sleep(2)\n",
    "        if (index % 25 == 0): print(\"Fetched till \", index)\n",
    "\n",
    "top250_df_copy.insert(loc=len(top250_df_copy.columns), column='YEAR', value=year_list)\n",
    "top250_df_copy.insert(loc=len(top250_df_copy.columns), column='Genre', value=all_genre_list)\n",
    "top250_df_copy.insert(loc=len(top250_df_copy.columns), column='IMDB_AWARDS_URL', value=awards_url_list)\n",
    "\n",
    "top250_df_copy.to_csv(\"/Users/gautamborgohain/Desktop/step1_imdb.csv\")\n",
    "\n",
    "oscar_nominations_list = []\n",
    "oscar_wins_list = []\n",
    "gg_nominations_list = []\n",
    "gg_wins_list = []\n",
    "sag_nominations_list = []\n",
    "sag_wins_list = []\n",
    "bafta_nominations_list = []\n",
    "bafta_wins_list = []\n",
    "cc_wins_list = []\n",
    "cc_nominations_list = []\n",
    "total_nominations_list = []\n",
    "total_wins_list = []\n",
    "\n",
    "\n",
    "def getCount(td):\n",
    "    return td.get('rowspan') if (td.get('rowspan')) else 0\n",
    "\n",
    "\n",
    "def toNumber(numberlist):\n",
    "    return numberlist[0] if (numberlist) else 0\n",
    "\n",
    "\n",
    "for index, row in top250_df_copy.iterrows():\n",
    "    temp_url = row['IMDB_AWARDS_URL']\n",
    "    if (temp_url != ' '):\n",
    "        oscar_win = 0\n",
    "        oscar_nom = 0\n",
    "        BAFTA_win = 0\n",
    "        BAFTA_nom = 0\n",
    "        gg_win = 0\n",
    "        gg_nom = 0\n",
    "        sag_win = 0\n",
    "        sag_nom = 0\n",
    "        cc_win = 0\n",
    "        cc_nom = 0\n",
    "        try:\n",
    "            temp_soup = getSoup(temp_url)\n",
    "            allstats = temp_soup.find('div', {'class': 'desc'})\n",
    "            total_wins = toNumber(re.findall(r'([0-9]*) wins', allstats.text))\n",
    "            total_nominations = toNumber(re.findall(r'([0-9]*) nominations', allstats.text))\n",
    "            td_award = temp_soup.findAll('td', {'class': 'title_award_outcome'})\n",
    "            for td in td_award:\n",
    "                if 'Won\\nOscar' in td.text:\n",
    "                    oscar_win = getCount(td)\n",
    "                elif 'Nominated\\nOscar' in td.text:\n",
    "                    oscar_nom = getCount(td)\n",
    "                elif 'Won\\nBAFTA' in td.text:\n",
    "                    BAFTA_win = getCount(td)\n",
    "                elif 'Nominated\\nBAFTA' in td.text:\n",
    "                    BAFTA_nom = getCount(td)\n",
    "                elif 'Won\\nGolden Globe' in td.text:\n",
    "                    gg_win = getCount(td)\n",
    "                elif 'Nominated\\nGolden Globe' in td.text:\n",
    "                    gg_nom = getCount(td)\n",
    "                elif 'Won\\nActor' in td.text:\n",
    "                    sag_win = getCount(td)\n",
    "                elif 'Nominated\\nActor' in td.text:\n",
    "                    sag_nom = getCount(td)\n",
    "                elif 'Won\\nCritics Choice' in td.text:\n",
    "                    cc_win = getCount(td)\n",
    "                elif 'Nominated\\nCritics Choice' in td.text:\n",
    "                    cc_nom = getCount(td)\n",
    "        except Exception as e:\n",
    "            print(\"Error at step 2\", e)\n",
    "            continue\n",
    "\n",
    "        finally:\n",
    "            oscar_wins_list.append(oscar_win)\n",
    "            oscar_nominations_list.append(oscar_nom)\n",
    "            cc_wins_list.append(cc_win)\n",
    "            cc_nominations_list.append(cc_nom)\n",
    "            bafta_wins_list.append(BAFTA_win)\n",
    "            bafta_nominations_list.append(BAFTA_nom)\n",
    "            gg_wins_list.append(gg_win)\n",
    "            gg_nominations_list.append(gg_nom)\n",
    "            sag_wins_list.append(sag_win)\n",
    "            sag_nominations_list.append(sag_nom)\n",
    "            total_nominations_list.append(total_nominations)\n",
    "            total_wins_list.append(total_wins)\n",
    "            if (index % 25 == 0): print(\"Fetched till \", index)\n",
    "\n",
    "movies_withoutAwards = top250_df_copy.loc[top250_df_copy.IMDB_AWARDS_URL == ' ']\n",
    "top250_df_copy = top250_df_copy.loc[\n",
    "    top250_df_copy.IMDB_AWARDS_URL != ' ']  # Remove the movies that did not have awards page\n",
    "top250_df_copy.insert(loc=len(top250_df_copy.columns), column='TOTAL_NOM', value=total_nominations_list)\n",
    "top250_df_copy.insert(loc=len(top250_df_copy.columns), column='TOTAL_WINS', value=total_wins_list)\n",
    "top250_df_copy.insert(loc=len(top250_df_copy.columns), column='OSCAR_NOM', value=oscar_nominations_list)\n",
    "top250_df_copy.insert(loc=len(top250_df_copy.columns), column='OSCAR_WIN', value=oscar_wins_list)\n",
    "top250_df_copy.insert(loc=len(top250_df_copy.columns), column='GG_NOM', value=gg_nominations_list)\n",
    "top250_df_copy.insert(loc=len(top250_df_copy.columns), column='GG_WINS', value=gg_wins_list)\n",
    "top250_df_copy.insert(loc=len(top250_df_copy.columns), column='BAFTA_NOM', value=bafta_nominations_list)\n",
    "top250_df_copy.insert(loc=len(top250_df_copy.columns), column='BAFTA_WIN', value=bafta_wins_list)\n",
    "top250_df_copy.insert(loc=len(top250_df_copy.columns), column='SAG_NOM', value=sag_nominations_list)\n",
    "top250_df_copy.insert(loc=len(top250_df_copy.columns), column='SAG_WINS', value=sag_wins_list)\n",
    "top250_df_copy.insert(loc=len(top250_df_copy.columns), column='CC_NOM', value=cc_nominations_list)\n",
    "top250_df_copy.insert(loc=len(top250_df_copy.columns), column='CC_WINS', value=cc_wins_list)\n",
    "\n",
    "top250_df_copy.to_csv(\"/Users/gautamborgohain/Desktop/step2_imdb.csv\")\n",
    "\n",
    "metaSearch_Start_url = 'http://www.metacritic.com/search/movie/'\n",
    "metaSearch_End_url = \"/results\"\n",
    "meta_home = \"http://www.metacritic.com/\"\n",
    "critic_score_list = []\n",
    "user_score_list = []\n",
    "movie_url_list = []\n",
    "for index, row in top250_df_copy.iterrows():\n",
    "    critic_score = 0\n",
    "    user_score = 0\n",
    "    movie_name = row['Title']\n",
    "    movie_name_encoded = quote(movie_name.encode('utf8'))\n",
    "    metaSearch_url = metaSearch_Start_url + movie_name_encoded + metaSearch_End_url\n",
    "    try:\n",
    "        results_soup = getSoup(metaSearch_url)\n",
    "        time.sleep(2)\n",
    "        firstResult = results_soup.find(text=movie_name)  # Search the results for the exact movie name\n",
    "        if (firstResult):\n",
    "            movie_home_url_part = firstResult.parent['href']\n",
    "            movie_home_url = meta_home + movie_home_url_part\n",
    "            movie_soup = getSoup(movie_home_url)  # Go to the movie's metacritic homepage\n",
    "            critic_shell = movie_soup.findAll('a', {\n",
    "                'class': 'metascore_anchor'})  # This gives a list of all the scores, we need the first two\n",
    "            if (critic_shell):\n",
    "                critic_score = cleanText(critic_shell[0].text)\n",
    "                user_score = cleanText(critic_shell[1].text)\n",
    "    except Exception as e:\n",
    "        print(\"Error at step 3\", e)\n",
    "        continue\n",
    "\n",
    "\n",
    "    finally:\n",
    "        movie_url_list.append(metaSearch_url)\n",
    "        critic_score_list.append(critic_score)\n",
    "        user_score_list.append(user_score)\n",
    "        time.sleep(2)\n",
    "        if (index % 25 == 0): print(\"Fetched till \", index)\n",
    "\n",
    "top250_df_copy.insert(loc=len(top250_df_copy.columns), column='MC_MOVIE_URL', value=movie_url_list)\n",
    "top250_df_copy.insert(loc=len(top250_df_copy.columns), column='MC_CRITIC_RATING', value=critic_score_list)\n",
    "top250_df_copy.insert(loc=len(top250_df_copy.columns), column='MC_USER_RATING', value=user_score_list)\n",
    "\n",
    "top250_df_copy.to_csv(\"/Users/gautamborgohain/Desktop/step3_mc.csv\")\n",
    "\n",
    "rotten_home = \"http://www.rottentomatoes.com\"\n",
    "rotten_search_start = \"http://www.rottentomatoes.com/search/?search=\"\n",
    "critic_score_list = []\n",
    "user_score_list = []\n",
    "movie_url_list = []\n",
    "\n",
    "def getMovieURL(movie_name,results_soup):\n",
    "    ul = results_soup.find('ul',{'id': 'movie_results_ul'})\n",
    "    if(ul):\n",
    "        lis = ul.findAll('li',{'class':'media bottom_divider clearfix'})\n",
    "        for li in lis:\n",
    "            if movie_name.lower() in li.text.lower().encode('ascii','ignore').decode('utf8'):\n",
    "                return li.find('a')['href']\n",
    "    return False\n",
    "\n",
    "for index, row in top250_df_copy.iterrows():\n",
    "    critic_score = 0\n",
    "    user_score = 0\n",
    "    movie_name = row['Title'].encode('ascii','ignore').decode('utf8')\n",
    "    movie_name_encoded = re.sub(' ','+',movie_name)\n",
    "    rtSearch_url = rotten_search_start+movie_name_encoded\n",
    "    results_soup = getSoup(rtSearch_url)\n",
    "    time.sleep(2)\n",
    "    if(results_soup):\n",
    "        resultsdiv = results_soup.findAll('div',{'id':'scoreStats'})\n",
    "        if(resultsdiv): # If it went to the Home Page directly\n",
    "            text = resultsdiv[0].text\n",
    "            critic_rating = toNumber(re.findall(r'Average Rating:  ([^/]+)',text))\n",
    "            resultsdiv = results_soup.findAll('div',{'class':'audience-info hidden-xs superPageFontColor'})\n",
    "            text = resultsdiv[0].text\n",
    "            user_rating = toNumber(re.findall(r'Average Rating: ([^/]+)',text))\n",
    "            critic_score_list.append(critic_rating)\n",
    "            user_score_list.append(user_rating)\n",
    "            movie_url_list.append(rtSearch_url)\n",
    "        else:\n",
    "            home_url = getMovieURL(movie_name,results_soup)\n",
    "            if(home_url):\n",
    "                complete_home_url = rotten_home+home_url\n",
    "                results_list= getSoup(complete_home_url)\n",
    "                resultsdiv = results_list.findAll('div',{'id':'scoreStats'})\n",
    "                if(len(resultsdiv)>0):\n",
    "                    text = resultsdiv[0].text\n",
    "                    critic_rating = toNumber(re.findall(r'Average Rating:  ([^/]+)',text))\n",
    "                    resultsdiv = results_list.findAll('div',{'class':'audience-info hidden-xs superPageFontColor'})\n",
    "                    text = resultsdiv[0].text\n",
    "                    user_rating = toNumber(re.findall(r'Average Rating: ([^/]+)',text))\n",
    "                    critic_score_list.append(critic_rating)\n",
    "                    user_score_list.append(user_rating)\n",
    "                    movie_url_list.append(complete_home_url)\n",
    "                else:\n",
    "                    print(\"Didnt find ratings info\", resultsdiv,complete_home_url)\n",
    "                    critic_score_list.append(0)\n",
    "                    user_score_list.append(0)\n",
    "                    movie_url_list.append(complete_home_url)\n",
    "            else:\n",
    "                print(\"Movie not in the result list\", home_url,rtSearch_url)\n",
    "                critic_score_list.append(0)\n",
    "                user_score_list.append(0)\n",
    "                movie_url_list.append(rtSearch_url)\n",
    "                \n",
    "    time.sleep(2)\n",
    "    if(index%25==0):print(\"Fetched till \",index)\n",
    "\n",
    "top250_df_copy.insert(loc = len(top250_df_copy.columns),column ='RT_MOVIE_URL',value = movie_url_list)\n",
    "top250_df_copy.insert(loc = len(top250_df_copy.columns),column ='RT_CRITIC_RATING',value = critic_score_list)\n",
    "top250_df_copy.insert(loc = len(top250_df_copy.columns),column ='RT_USER_RATING',value = user_score_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
